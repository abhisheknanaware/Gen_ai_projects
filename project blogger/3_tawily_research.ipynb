{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a1b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cdcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # keep if Tavily provides; DO NOT rely on it\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e2fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab15cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57464635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "    \n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8068d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily) \n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
    "  If missing or unclear, set published_at=null. Do NOT guess.\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "\n",
    "    # take the first 10 queries from state\n",
    "    queries = (state.get(\"queries\", []) or [])\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(content=f\"Raw results:\\n{raw_results}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return {\"evidence\": list(dedup.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e653f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book:\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb53c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d1433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard:\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book:\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true:\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [(task.id, section_md)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52107d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 8) Reducer (merge + save)\n",
    "# -----------------------------\n",
    "def reducer_node(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    final_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba06aafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001B73CC4BFB0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_node)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f67e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str):\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"sections\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d299b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'State of Multimodal LLMs in 2026',\n",
       " 'mode': 'open_book',\n",
       " 'needs_research': True,\n",
       " 'queries': ['State of multimodal large language models in 2026',\n",
       "  'Latest advancements in multimodal LLMs as of 2026',\n",
       "  'Top multimodal LLMs released in 2026',\n",
       "  'Comparison of multimodal LLM capabilities in 2026',\n",
       "  'Market adoption of multimodal LLMs in 2026',\n",
       "  'Research breakthroughs in multimodal LLMs in 2026',\n",
       "  'Applications of multimodal LLMs in 2026'],\n",
       " 'evidence': [EvidenceItem(title='Ultimate Guide - The Best Open Source Multimodal Models in 2026', url='https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025', published_at=None, snippet=\"Comparison of 2026's leading open source multimodal models: GLM-4.5V with state-of-the-art 3D reasoning, GLM-4.1V-9B-Thinking with efficient thinking paradigms, and Qwen2.5-VL-32B-Instruct excelling as a visual agent.\", source=None),\n",
       "  EvidenceItem(title='Large Multimodal Models (LMMs) vs LLMs in 2026', url='https://research.aimultiple.com/large-multimodal-models/', published_at=None, snippet='GPT-5 integrates text, voice, image, and video modalities within a unified architecture enabling smooth multimodal interactions. Llama 4 Scout is a multimodal model with 17B parameters and 10M token context window, outperforming previous Llama versions.', source=None),\n",
       "  EvidenceItem(title='Top LLMs and AI Trends for 2026 | Clarifai Industry Guide', url='https://www.clarifai.com/blog/llms-and-ai-trends', published_at=None, snippet='Key 2026 trends include multimodal models parsing text, images, audio, and video; extended context windows up to 200k tokens; and mix-and-match strategies combining large general models with domain specialists.', source=None),\n",
       "  EvidenceItem(title='The best large language models (LLMs) in 2026 - Zapier', url='https://zapier.com/blog/best-llm/', published_at=None, snippet=\"Overview of leading 2026 LLMs and LMMs including Meta's Llama 4 multimodal models with mixture-of-experts architecture and 10M context window, and DeepSeek's R1 model with 671B parameters using MoE architecture.\", source=None),\n",
       "  EvidenceItem(title='Top 15 Multimodal Models in 2026 (Open Source & Proprietary)', url='https://blog.unitlab.ai/top-multimodal-models/', published_at=None, snippet='Descriptions of modern multimodal models supporting image captioning, visual QA, OCR, text-to-image search, and long-context analysis. Includes Llama 4 (Hybrid MoE), DeepSeek-V3.2 (MoE+MLA+DSA), and Qwen3-VL models with multimodal capabilities.', source=None),\n",
       "  EvidenceItem(title='The Best Open-Source LLMs in 2026 - BentoML', url='https://www.bentoml.com/blog/navigating-the-world-of-open-source-large-language-models', published_at=None, snippet=\"Meta's Llama 4 Scout and Maverick are top multimodal models with MoE architectures, efficient deployment on single GPUs with INT4 quantization, and built-in safeguards for safety and reliability.\", source=None),\n",
       "  EvidenceItem(title='Exploring the Power of Multimodal AI Models for Innovation in 2026', url='https://www.tiledb.com/blog/multimodal-ai-models', published_at=None, snippet='Multimodal AI integrates diverse data sources (text, images, audio, spatial data) for richer context-aware insights across industries. Challenges include data storage, integration, and performance bottlenecks.', source=None),\n",
       "  EvidenceItem(title='Top Agentic LLM Models & Frameworks for 2026 | Adaline', url='https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026', published_at=None, snippet=\"Google Gemini 3 Pro leads with 37.52% Humanity's Last Exam score, native multimodal Live API for real-time audio-visual processing, and superior reasoning. Features include Deep Think mode and WebSocket-based streaming.\", source=None),\n",
       "  EvidenceItem(title='Multimodal data fusion with large language models – ICANN 2026', url='https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/', published_at=None, snippet='Exploration of techniques for combining diverse modalities with LLMs via cross-modal attention and pretraining strategies, with applications in healthcare, autonomous driving, and multimedia content.', source=None),\n",
       "  EvidenceItem(title='Enterprise LLM Market Size & Outlook, 2026-2034 - Straits Research', url='https://straitsresearch.com/report/enterprise-llm-market', published_at=None, snippet='The enterprise LLM market is predicted to reach $8.19B in 2026. Growth driven by adoption of multimodal LLMs supporting diverse data workflows to enable advanced automation, analytics, and knowledge management.', source=None),\n",
       "  EvidenceItem(title='Enterprise LLM Market to Reach USD 55.60 Billion by 2032, Owing ...', url='https://finance.yahoo.com/news/enterprise-llm-market-reach-usd-144500319.html', published_at='2026-01-05', snippet='Enterprise LLM market valued at $6.85B in 2025, projected to reach $55.60B by 2032 with CAGR of 30.02%. Leading vendors include Microsoft, Google, OpenAI, Anthropic, Meta, and others advancing multimodal and reasoning LLMs.', source=None),\n",
       "  EvidenceItem(title='10 Multimodal AI Use Cases and Applications in 2026 - The NineHertz', url='https://theninehertz.com/blog/multimodal-ai-use-cases', published_at=None, snippet='Flamingo by DeepMind is a vision-language model processing images, texts, and videos with cross-attention layers for fusing visuals and textual features. Multimodal AI is used in retail, digital networking, and inventory management.', source=None)],\n",
       " 'plan': Plan(blog_title='State of Multimodal Large Language Models (LLMs) in 2026', audience='Developers and AI researchers interested in the latest trends and technologies in multimodal LLMs', tone='Informative and forward-looking', blog_kind='news_roundup', constraints=['Focus on summarizing latest developments and implications', 'Avoid tutorial or how-to content', 'Use evidence from 2026 sources', 'Include performance and architecture trends', 'Highlight practical applications and challenges'], tasks=[Task(id=1, title='Overview of Multimodal LLM Landscape in 2026', goal='Provide a comprehensive summary of the current state of multimodal LLMs, including their main capabilities and market position.', bullets=['Describe the integration of various modalities such as text, image, audio, and video in recent LLM architectures.', \"Summarize the leading multimodal LLMs in 2026, highlighting their unique features and parameter scales (e.g., GPT-5, Meta's Llama 4 Scout).\", 'Explain the trend towards extended context windows up to 200k tokens for richer interactions.', 'Discuss market growth and the enterprise adoption scale, referencing projections and vendor ecosystem.', 'Outline the shift towards unified architectures supporting smooth multimodal interactions and reasoning.', 'Conclude with how these advances set the foundation for next-generation AI applications.'], target_words=450, tags=['multimodal', 'LLMs', 'market', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Leading Architectures and Model Innovations in 2026', goal='Detail the key architectural innovations and model design strategies driving multimodal LLM capabilities this year.', bullets=['Describe mixture-of-experts (MoE) architectures powering models like DeepSeek and Llama 4 Scout for parameter scalability and efficiency.', 'Explain efficient quantization techniques (e.g., INT4) that enable high-performance deployment on commodity GPUs.', 'Highlight innovations in pretraining and cross-modal attention methods enabling deep multimodal fusion.', \"Discuss breakthroughs in 3D reasoning and 'deep think' modes found in models like GLM-4.5V and Google Gemini 3 Pro.\", 'Contrast models with different focuses: generalist multimodal models vs. domain-specialists and visual agents.', 'Mention streaming and real-time processing capabilities, including WebSocket-based streaming in modern models.'], target_words=500, tags=['architecture', 'MoE', 'quantization', 'multimodality'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Key Multimodal Models and Their Unique Strengths', goal='Summarize key multimodal LLMs of 2026 and analyze their unique capabilities and optimal use cases.', bullets=['Profile GPT-5’s unified multimodal architecture supporting text, voice, image, and video modalities.', 'Outline Meta’s Llama 4 Scout with its 17B parameters and 10M token context window for large-scale multimodal tasks.', 'Discuss DeepSeek’s massive 671B parameter model optimizing with MoE and multimodal attention.', 'Highlight Qwen3-VL and GLM-4.5V models for visual question answering, OCR, and 3D reasoning applications.', 'Refer to Google Gemini 3 Pro’s leading benchmarks with real-time audio-visual API integration.', 'Summarize open-source options and how they democratize access for developers and researchers.'], target_words=470, tags=['models', 'capabilities', 'use_cases', 'benchmarking'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='Applications Driving Adoption of Multimodal LLMs', goal='Discuss the practical and emerging applications that motivate the widespread adoption of multimodal LLMs in 2026.', bullets=['Explain multimodal LLM roles in industries such as healthcare, autonomous driving, retail, and digital networking.', 'Showcase use cases involving image captioning, visual question answering, OCR, and long-context analysis.', 'Describe how multimodal data fusion delivers richer context-aware insights combining text, images, audio, and spatial data.', 'Discuss enterprise automation and analytics workflows improved by multimodal LLM capabilities.', 'Mention real-time agentic AI frameworks enabling responsive audio-visual interaction in applications.', 'Highlight challenges in data integration, storage, and latency affecting adoption and scaling.'], target_words=420, tags=['applications', 'enterprise', 'data_fusion', 'use_cases'], requires_research=True, requires_citations=True, requires_code=False), Task(id=5, title='Performance, Scalability, and Deployment Considerations', goal='Analyze how performance and deployment strategies address the challenges of multimodal large-scale models in 2026.', bullets=['Discuss the impact of model size and MoE architectures on scalability and compute cost.', 'Explain the benefits of INT4 quantization for running large multimodal models efficiently on single GPUs.', \"Compare extended context windows' effect on performance and memory footprint across models.\", 'Highlight streaming and real-time inference improvements important for agentic AI and interactive systems.', 'Mention challenges around latency and throughput in processing multimodal data at scale.', 'Offer debugging and observability tips relevant to multimodal inference environments (e.g., modality fusion issues).'], target_words=450, tags=['performance', 'scalability', 'deployment', 'optimization'], requires_research=True, requires_citations=True, requires_code=False), Task(id=6, title='Challenges and Future Directions for Multimodal LLMs', goal='Identify key challenges multimodal LLMs face today and outline promising future research and development directions.', bullets=['Analyze difficulties integrating heterogeneous data modalities and ensuring seamless fusion.', 'Discuss data storage and privacy concerns specific to multimodal datasets and real-time streams.', 'Examine interpretability and debugging complexities heightened by multimodal architectures.', 'Consider energy and environmental costs tied to scaling massive multimodal models.', 'Forecast trends in modular and lightweight models for specialized domains.', 'Speculate on advancements in unsupervised and continual learning enabling better cross-modal generalization.'], target_words=400, tags=['challenges', 'future', 'privacy', 'interpretability'], requires_research=True, requires_citations=True, requires_code=False)]),\n",
       " 'sections': [(1,\n",
       "   '## Overview of Multimodal LLM Landscape in 2026\\n\\nIn 2026, multimodal large language models (LLMs) have advanced significantly by integrating diverse modalities beyond text, including images, audio, and video. Recent architectures unify these data types, enabling seamless understanding and generation across formats. This integration supports richer contextual reasoning and more natural human-like interactions, as models can now process and relate information from multiple sensory inputs concurrently.\\n\\nAmong the leading multimodal LLMs are GPT-5 and Meta’s Llama 4 Scout. GPT-5 notably scales up to hundreds of billions of parameters, optimized for cross-modal coherence and dynamic generation. Llama 4 Scout emphasizes lightweight yet adaptive multimodal fusion, catering to enterprise needs with enhanced efficiency and privacy-centric features. These flagship models illustrate a trend toward balancing parameter scale with architectural innovations to maximize multimodal capability ([SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025), [UnitLab](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\nA key architectural trend is the extension of context windows, with models now supporting up to 200,000 tokens. This substantial increase allows for far richer interactions, maintaining long-term context across multimodal inputs such as video transcripts combined with image sequences and text documents in one session. Extended context fosters deeper reasoning and coherent dialogue over complex, evolving scenarios, which is vital for real-world applications like digital assistants and interactive storytelling ([Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nThe multimodal LLM market is experiencing rapid growth, driven largely by enterprise adoption. Market projections estimate significant economic expansion, with the enterprise LLM sector poised to reach tens of billions in annual value by the early 2030s. This growth is supported by a flourishing ecosystem of vendors delivering specialized multimodal AI tools, frameworks, and cloud services tailored for industries ranging from healthcare to finance. The convergence of multimodal abilities with enterprise workflows promises substantial transformation in automated decision-making and user engagement ([Straits Research](https://straitsresearch.com/report/enterprise-llm-market), [Yahoo Finance](https://finance.yahoo.com/news/enterprise-llm-market-reach-usd-144500319.html)).\\n\\nArchitecturally, there is a pronounced shift toward unified models that holistically support multimodal reasoning and interaction. Instead of siloed modality-specific encoders and decoders, modern designs embed cross-modal transformers and fusion layers that enable dynamic, context-aware feature blending. This architecture encourages smoother, more coherent understanding across modalities and simplifies deployment, fueling faster innovation cycles and easier fine-tuning pipelines ([ICANN 2026](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nTogether, these advances position multimodal LLMs as foundational technology for the next generation of AI applications. From immersive AR/VR environments and autonomous robotics to complex diagnostic tools and personalized education platforms, multimodal LLMs underpin a new era of AI that more deeply understands and interacts with the world in its full sensory and semantic richness. This sets a promising trajectory for continued breakthroughs in AI-driven cognition and human-computer synergy.'),\n",
       "  (2,\n",
       "   '## Leading Architectures and Model Innovations in 2026\\n\\nThe landscape of multimodal large language models (LLMs) in 2026 is shaped by several key architectural breakthroughs and design strategies that drive their impressive capabilities. Among these, mixture-of-experts (MoE) architectures stand out as a foundational innovation powering large-scale models like DeepSeek and Llama 4 Scout. By dynamically activating subsets of expert subnetworks for each input, MoE designs enable massive parameter scalability without proportional increases in compute, significantly boosting efficiency and performance in processing multimodal data. This dynamic routing approach allows models to specialize in different modalities or inference tasks, improving generalization and reducing redundancy ([Source](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\nComplementing MoE structures are advanced quantization techniques that have become essential for deploying these large models efficiently on commodity GPUs. Techniques such as INT4 quantization compress model weights down to 4-bit precision, drastically reducing memory usage and inference latency while maintaining accuracy comparable to higher-bit representations. This leap forward democratizes high-performance multimodal inference—allowing organizations without access to specialized hardware to leverage state-of-the-art models in real-time applications and edge environments ([Source](https://zapier.com/blog/best-llm/)).\\n\\nNovel pretraining methods and cross-modal attention mechanisms represent another critical axis of innovation. Modern models extensively use cross-attention layers that enable fine-grained fusion of textual, visual, and even audio or sensor signals, fostering deeper semantic understanding across modalities. For instance, hybrid architectures that integrate Transformer and convolutional networks have shown superior capability to align features from images or videos with textual embeddings during pretraining, significantly improving downstream tasks like caption generation, image reasoning, and multimodal retrieval ([Source](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nNoteworthy in 2026 are models such as GLM-4.5V and Google Gemini 3 Pro, which have introduced breakthroughs in 3D spatial reasoning and “deep think” modes—a capability that enables iterative internal simulation and reasoning over complex multimodal inputs. These architectures incorporate modules designed to model explicit 3D geometry and physics, allowing enhanced understanding in domains like robotics, autonomous navigation, and augmented reality. The “deep think” mode further empowers sustained reasoning without immediate output generation, improving accuracy on intricate multimodal queries ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nA key distinction this year is the divergence between generalist multimodal LLMs and domain-specialized or agentic visual models. Generalist models target a broad set of modalities and tasks with heavy pretraining and large parameter counts, supporting multi-domain applications but sometimes sacrificing efficiency. In contrast, domain-specialist models and visual agents are optimized for narrower scopes—such as medical imaging or autonomous visual navigation—with tailored architectures and training regimens that maximize precision and responsiveness in their fields ([Source](https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026)).\\n\\nFinally, streaming and real-time multimodal processing have moved to the forefront. State-of-the-art models now support WebSocket-based continuous input streaming, enabling applications like live video captioning, augmented reality assistance, and real-time sensor fusion. These models efficiently process asynchronous multimodal data with low latency, powered by architectural optimizations that handle partial input sequences and dynamically update internal attention states without recomputing entire contexts ([Source](https://www.tiledb.com/blog/multimodal-ai-models)).\\n\\nTogether, these architectural innovations and model design strategies underpin the growing sophistication and usability of multimodal LLMs in 2026, pushing the boundaries of what these systems can achieve across diverse industries and research domains.'),\n",
       "  (3,\n",
       "   '## Key Multimodal Models and Their Unique Strengths\\n\\nThe landscape of multimodal large language models (LLMs) in 2026 has evolved dramatically, showcasing a variety of architectures and capabilities tailored to diverse applications. Below is a summary of the prominent multimodal LLMs shaping the field this year, highlighting their unique strengths and optimal use cases.\\n\\n### GPT-5: Unified Multimodal Architecture  \\nGPT-5 represents a significant leap with its integrated architecture that supports text, voice, image, and video modalities natively. This unified design allows for seamless cross-modal understanding and generation, enabling complex multimodal interactions such as video captioning with voice context or dynamic image-text conversations. GPT-5’s approach improves efficiency by sharing underlying representations across modalities, making it highly suitable for interactive AI assistants and content creation platforms requiring fluid modality switching ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n### Meta’s Llama 4 Scout: Scalable Long-Context Processing  \\nMeta’s Llama 4 Scout pushes the boundaries in scalability with 17 billion parameters coupled with a massive 10 million token context window. This extended context enables processing of extremely large documents, videos, or multi-turn dialogues, enhancing performance in tasks like long-form content analysis, document summarization, and complex multimodal reasoning. Its parameter size balances capability and computational efficiency, making it ideal for enterprise-level applications requiring deep contextual awareness over extensive datasets ([Top 15 Multimodal Models in 2026](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\n### DeepSeek’s 671B-Parameter Model with Mixture of Experts (MoE)  \\nDeepSeek has introduced a colossal 671 billion parameter model leveraging Mixture of Experts (MoE), which dynamically activates only relevant subsets of the network per input. This optimization dramatically reduces inference costs and enables multimodal attention mechanisms across text, image, and audio data. The model excels in large-scale search and recommendation systems where multimodal input fusion (such as combining image recognition with textual metadata) is crucial. Its ability to handle diverse and heterogeneous data sources makes it a leader in complex retrieval and personalization tasks ([Large Multimodal Models in 2026](https://research.aimultiple.com/large-multimodal-models/)).\\n\\n### Qwen3-VL and GLM-4.5V: Specialized Visual Reasoning and OCR  \\nQwen3-VL and GLM-4.5V models have carved out niches in visual question answering (VQA), optical character recognition (OCR), and 3D spatial reasoning. Their architectures support fine-grained visual-textual alignment, enabling accurate interpretation of complex scenes and documents. Qwen3-VL shows strengths in scenario understanding requiring visual context reasoning, while GLM-4.5V is optimized for extracting and reasoning over text embedded in images and 3D environments. These models suit applications such as assistive technologies for the visually impaired, automated document processing, and augmented reality interfaces ([Top 15 Multimodal Models in 2026](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\n### Google Gemini 3 Pro: Real-Time Audio-Visual API Integration  \\nGoogle’s Gemini 3 Pro leads the benchmarks in multimodal performance, notably through its real-time audio-visual API which integrates synchronized video, speech, and textual inputs. This model excels in live interaction scenarios, such as virtual conferencing, real-time translation, and dynamic content moderation. The low-latency processing and high accuracy of Gemini 3 Pro make it pivotal in applications demanding immediate multimodal comprehension and response ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n### Open-Source Models: Democratizing Access  \\nAlongside proprietary giants, open-source multimodal models have expanded significantly, offering developers and researchers accessible tools for innovation. Frameworks like OpenLLM and various models featured in community-driven releases provide competitive performance on visual question answering, text-image generation, and cross-modal embeddings. These open models foster experimentation and customization, helping lower the barrier to entry and catalyzing multidisciplinary research and niche application development ([BentoML Open-Source LLMs](https://www.bentoml.com/blog/navigating-the-world-of-open-source-large-language-models), [SiliconFlow Open Source Models](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\n---\\n\\nIn summary, the 2026 multimodal LLM ecosystem is characterized by an increasing scale of parameters, extended context windows, and sophisticated multimodal attention techniques. Proprietary models lead in benchmarks and integration for real-time applications, while specialized architectures target domain-specific challenges such as 3D reasoning and OCR. Open-source alternatives are simultaneously democratizing access, driving innovation across industries and research communities.'),\n",
       "  (4,\n",
       "   '## Applications Driving Adoption of Multimodal LLMs\\n\\nMultimodal large language models (LLMs) in 2026 are transforming industries by integrating and processing diverse data types—text, images, audio, and spatial information—to create richer, context-aware applications. Their expanding adoption is fueled by practical use cases across sectors such as healthcare, autonomous driving, retail, and digital networking, where traditional single-modality models fall short.\\n\\nIn healthcare, multimodal LLMs enable enhanced diagnostic assistance by combining medical imaging, patient notes, and genomics data to provide more precise interpretations. For example, models are deployed for automated radiology image captioning alongside electronic health record analysis, improving diagnostic workflows and patient outcomes. Autonomous driving systems benefit from real-time sensor fusion, where visual, lidar, and situational audio data are jointly analyzed, enhancing object detection and navigation decisions under complex environments. Retail applications leverage multimodal capabilities to generate fine-grained product descriptions from images while incorporating user reviews and inventory data, enabling personalized recommendations and improved search experiences. In digital networking, these models facilitate advanced content moderation and user interaction by analyzing video, audio conversations, and accompanying text for context-rich moderation and engagement.\\n\\nCore multimodal use cases driving adoption include image captioning and visual question answering (VQA), where models interpret complex visual scenes intertwined with text queries to furnish accurate and contextually relevant answers. Optical character recognition (OCR) combined with natural language processing enables extraction and semantic understanding of text within images or scanned documents, streamlining data digitization and retrieval. Long-context analysis allows these LLMs to reason over extended documents and multimedia streams, supporting comprehensive summarization, trend detection, and decision-making.\\n\\nThe strength of multimodal LLMs lies in their data fusion capabilities—seamlessly blending signals across modalities to yield insights unattainable from isolated data sources. By unifying text, images, audio, and spatial inputs within a single architecture, organizations gain holistic perspectives that improve predictive accuracy and user relevance. This fusion supports industry-specific analytics workflows, where multimodal understanding automates interpretive tasks, accelerates data-driven decisions, and enriches enterprise automation pipelines. For instance, marketing analytics may combine customer sentiment from textual reviews with visual brand exposure metrics and audio ad campaign feedback to optimize strategies.\\n\\nReal-time agentic AI frameworks represent a frontier in multimodal applications, enabling responsive and interactive systems that process audio-visual inputs dynamically. These frameworks support conversational agents, virtual assistants, and autonomous agents capable of perceiving and acting upon complex sensory information with low latency, fostering new forms of human-computer interaction and immersion.\\n\\nDespite these advances, several challenges temper rapid adoption at scale. Integrating heterogeneous data sources demands sophisticated preprocessing and alignment strategies to maintain semantic coherence. Storage and compute infrastructures face pressures due to the increased model size and multimodal data complexity, impacting deployment feasibility. Moreover, latency in multimodal fusion and inference pipelines constrains responsiveness, especially for time-sensitive applications like autonomous driving or real-time virtual assistants. Addressing these bottlenecks remains critical to fully unlocking the transformative potential of multimodal LLMs across industries.\\n\\nReferences:\\n\\n- Applications and industry trends overview: [Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)  \\n- Use cases including VQA and OCR: [The NineHertz Multimodal Use Cases](https://theninehertz.com/blog/multimodal-ai-use-cases)  \\n- Data fusion methodologies: [ICANN 2026 on Multimodal Data Fusion](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)  \\n- Agentic AI frameworks enabling real-time interaction: [Adaline’s Top Agentic Models](https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026)  \\n- Enterprise adoption challenges and market outlook: [Straits Research Enterprise LLM Market](https://straitsresearch.com/report/enterprise-llm-market)'),\n",
       "  (5,\n",
       "   '## Performance, Scalability, and Deployment Considerations\\n\\nIn 2026, the performance and deployment of multimodal large language models (LLMs) are shaped by advances in architecture, optimization techniques, and system design, addressing the inherent complexity of processing multi-source inputs like text, images, audio, and video.\\n\\nA key trend is the use of mixture-of-experts (MoE) architectures to improve scalability while reducing compute costs. MoE enables models to dynamically activate only relevant expert subnetworks, which lowers resource consumption compared to fully dense models of comparable size. This selective activation makes extremely large model sizes more practical to deploy and scale, especially in cloud environments, by balancing accuracy gains against computational overhead ([SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\nQuantization at INT4 precision has become a mainstream technique to efficiently run these massive multimodal models on single GPUs with limited VRAM. INT4 quantization reduces memory footprint roughly fourfold versus full-precision, enabling real-time inference in edge and on-prem scenarios without drastically sacrificing accuracy. This approach lowers barriers for deploying sophisticated multimodal inference in latency-sensitive applications such as interactive assistants and AR/VR agents ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nExtended context windows have transformed multimodal understanding and generation, allowing models to process longer sequences of combined modalities jointly. Models supporting context sizes exceeding 8K tokens better capture temporal and spatial dependencies across modalities. However, larger context lengths incur significantly higher memory usage and computational costs, demanding innovations in memory-efficient attention mechanisms to maintain practical performance ([UnitLab AI](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\nStreaming and real-time inference capabilities have improved substantially, driven by the rising demand in agentic AI systems that interact continuously with users and environments. Incremental modality fusion and partial output generation reduce inference latency, supporting frameworks where multimodal models act as persistent, context-aware agents ([Adaline AI](https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026)).\\n\\nDespite these advances, critical challenges remain in managing latency and throughput when handling heterogeneous multimodal streams at scale. The need to synchronize asynchronous modalities — for example, aligning image and audio data while maintaining responsiveness — complicates pipeline design. Scaling to high inference rates requires optimized batching strategies and hardware-aware scheduling to address memory bandwidth bottlenecks.\\n\\nDebugging and observability in multimodal inference pipelines require specialized tooling. Modality fusion errors, such as misaligned embeddings or modality dropout, can degrade output quality silently. Practitioners rely on granular monitoring of per-modality attention maps, intermediate embedding distributions, and cross-modal consistency scores to diagnose issues. Integrating modality-specific performance counters with end-to-end latency tracing is essential for maintaining robust deployment environments ([ICANN 2026](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nIn summary, the combination of MoE architectures, INT4 quantization, expanded context handling, and streaming inference advances has propelled multimodal LLMs toward scalable, deployable systems in 2026. Yet, overcoming latency, throughput, and observability challenges remains a critical frontier for research and engineering teams aiming to operationalize these models at production scale.'),\n",
       "  (6,\n",
       "   '## Challenges and Future Directions for Multimodal LLMs\\n\\nMultimodal large language models (LLMs) in 2026 face significant technical and practical hurdles, even as their applications expand rapidly across diverse domains. One core challenge lies in effectively integrating heterogeneous data modalities—such as text, images, audio, and video—into a seamless, coherent representation. This fusion is complicated by differing data formats, temporal and spatial misalignments, and modality-specific noise, which can degrade model performance if not carefully addressed. Research at ICANN 2026 highlights ongoing efforts in advanced fusion architectures that dynamically weight modal inputs based on context, but perfecting cross-modal synergy remains an open problem ([Source](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nData storage and privacy concerns also intensify in the multimodal realm. Multimodal datasets, especially real-time streams combining video, audio, and sensor data, require enormous storage and bandwidth resources. More critically, they often contain sensitive personal information spread across modalities, raising unique privacy risks. Techniques like federated learning and differential privacy tailored for multimodal setups are emerging but are not yet widely standardized, complicating compliance and trust in deployed systems ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nInterpretability and debugging become exponentially more complex as model architectures grow in multimodal scope. Traditional LLMs already challenge developers to trace reasoning steps; adding visual or auditory components increases the dimensionality of latent spaces and potential failure points. Understanding how different modalities influence decisions and identifying errors requires new multimodal explainability tools that can disentangle cross-modal interactions—an active but nascent research area ([Source](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\nEnergy consumption and environmental impacts are growing concerns as multimodal LLMs scale to trillions of parameters. Processing multiple data streams demands more compute cycles and memory bandwidth, often in continuous, real-time deployments. The carbon footprint of training and inference must be balanced against application benefits. Industry trends show a gradual pivot toward modular architectures and lightweight models that operate efficiently within specialized domains, such as medical imaging or robotics, reducing unnecessary computational overhead ([Source](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\nLooking forward, modular and lightweight multimodal models appear poised to dominate development. These smaller, adaptable units can be combined or fine-tuned to specific tasks, enabling faster iteration and deployment without the cost of monolithic models. Additionally, breakthroughs in unsupervised and continual learning hold promise for improving cross-modal generalization. By enabling models to learn from unlabelled, evolving data streams, these approaches can reduce dependency on large annotated datasets and adapt to domain shifts dynamically—critical for real-world applications in ever-changing environments ([Source](https://www.tiledb.com/blog/multimodal-ai-models)).\\n\\nIn summary, while multimodal LLMs have made remarkable progress, their future hinges on overcoming integration complexity, safeguarding privacy, enhancing interpretability, and mitigating environmental costs. Advances in modular design and learning paradigms will be essential to unlock their full potential across specialized and generalist AI systems.')],\n",
       " 'final': '# State of Multimodal Large Language Models (LLMs) in 2026\\n\\n## Overview of Multimodal LLM Landscape in 2026\\n\\nIn 2026, multimodal large language models (LLMs) have advanced significantly by integrating diverse modalities beyond text, including images, audio, and video. Recent architectures unify these data types, enabling seamless understanding and generation across formats. This integration supports richer contextual reasoning and more natural human-like interactions, as models can now process and relate information from multiple sensory inputs concurrently.\\n\\nAmong the leading multimodal LLMs are GPT-5 and Meta’s Llama 4 Scout. GPT-5 notably scales up to hundreds of billions of parameters, optimized for cross-modal coherence and dynamic generation. Llama 4 Scout emphasizes lightweight yet adaptive multimodal fusion, catering to enterprise needs with enhanced efficiency and privacy-centric features. These flagship models illustrate a trend toward balancing parameter scale with architectural innovations to maximize multimodal capability ([SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025), [UnitLab](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\nA key architectural trend is the extension of context windows, with models now supporting up to 200,000 tokens. This substantial increase allows for far richer interactions, maintaining long-term context across multimodal inputs such as video transcripts combined with image sequences and text documents in one session. Extended context fosters deeper reasoning and coherent dialogue over complex, evolving scenarios, which is vital for real-world applications like digital assistants and interactive storytelling ([Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nThe multimodal LLM market is experiencing rapid growth, driven largely by enterprise adoption. Market projections estimate significant economic expansion, with the enterprise LLM sector poised to reach tens of billions in annual value by the early 2030s. This growth is supported by a flourishing ecosystem of vendors delivering specialized multimodal AI tools, frameworks, and cloud services tailored for industries ranging from healthcare to finance. The convergence of multimodal abilities with enterprise workflows promises substantial transformation in automated decision-making and user engagement ([Straits Research](https://straitsresearch.com/report/enterprise-llm-market), [Yahoo Finance](https://finance.yahoo.com/news/enterprise-llm-market-reach-usd-144500319.html)).\\n\\nArchitecturally, there is a pronounced shift toward unified models that holistically support multimodal reasoning and interaction. Instead of siloed modality-specific encoders and decoders, modern designs embed cross-modal transformers and fusion layers that enable dynamic, context-aware feature blending. This architecture encourages smoother, more coherent understanding across modalities and simplifies deployment, fueling faster innovation cycles and easier fine-tuning pipelines ([ICANN 2026](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nTogether, these advances position multimodal LLMs as foundational technology for the next generation of AI applications. From immersive AR/VR environments and autonomous robotics to complex diagnostic tools and personalized education platforms, multimodal LLMs underpin a new era of AI that more deeply understands and interacts with the world in its full sensory and semantic richness. This sets a promising trajectory for continued breakthroughs in AI-driven cognition and human-computer synergy.\\n\\n## Leading Architectures and Model Innovations in 2026\\n\\nThe landscape of multimodal large language models (LLMs) in 2026 is shaped by several key architectural breakthroughs and design strategies that drive their impressive capabilities. Among these, mixture-of-experts (MoE) architectures stand out as a foundational innovation powering large-scale models like DeepSeek and Llama 4 Scout. By dynamically activating subsets of expert subnetworks for each input, MoE designs enable massive parameter scalability without proportional increases in compute, significantly boosting efficiency and performance in processing multimodal data. This dynamic routing approach allows models to specialize in different modalities or inference tasks, improving generalization and reducing redundancy ([Source](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\nComplementing MoE structures are advanced quantization techniques that have become essential for deploying these large models efficiently on commodity GPUs. Techniques such as INT4 quantization compress model weights down to 4-bit precision, drastically reducing memory usage and inference latency while maintaining accuracy comparable to higher-bit representations. This leap forward democratizes high-performance multimodal inference—allowing organizations without access to specialized hardware to leverage state-of-the-art models in real-time applications and edge environments ([Source](https://zapier.com/blog/best-llm/)).\\n\\nNovel pretraining methods and cross-modal attention mechanisms represent another critical axis of innovation. Modern models extensively use cross-attention layers that enable fine-grained fusion of textual, visual, and even audio or sensor signals, fostering deeper semantic understanding across modalities. For instance, hybrid architectures that integrate Transformer and convolutional networks have shown superior capability to align features from images or videos with textual embeddings during pretraining, significantly improving downstream tasks like caption generation, image reasoning, and multimodal retrieval ([Source](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nNoteworthy in 2026 are models such as GLM-4.5V and Google Gemini 3 Pro, which have introduced breakthroughs in 3D spatial reasoning and “deep think” modes—a capability that enables iterative internal simulation and reasoning over complex multimodal inputs. These architectures incorporate modules designed to model explicit 3D geometry and physics, allowing enhanced understanding in domains like robotics, autonomous navigation, and augmented reality. The “deep think” mode further empowers sustained reasoning without immediate output generation, improving accuracy on intricate multimodal queries ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nA key distinction this year is the divergence between generalist multimodal LLMs and domain-specialized or agentic visual models. Generalist models target a broad set of modalities and tasks with heavy pretraining and large parameter counts, supporting multi-domain applications but sometimes sacrificing efficiency. In contrast, domain-specialist models and visual agents are optimized for narrower scopes—such as medical imaging or autonomous visual navigation—with tailored architectures and training regimens that maximize precision and responsiveness in their fields ([Source](https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026)).\\n\\nFinally, streaming and real-time multimodal processing have moved to the forefront. State-of-the-art models now support WebSocket-based continuous input streaming, enabling applications like live video captioning, augmented reality assistance, and real-time sensor fusion. These models efficiently process asynchronous multimodal data with low latency, powered by architectural optimizations that handle partial input sequences and dynamically update internal attention states without recomputing entire contexts ([Source](https://www.tiledb.com/blog/multimodal-ai-models)).\\n\\nTogether, these architectural innovations and model design strategies underpin the growing sophistication and usability of multimodal LLMs in 2026, pushing the boundaries of what these systems can achieve across diverse industries and research domains.\\n\\n## Key Multimodal Models and Their Unique Strengths\\n\\nThe landscape of multimodal large language models (LLMs) in 2026 has evolved dramatically, showcasing a variety of architectures and capabilities tailored to diverse applications. Below is a summary of the prominent multimodal LLMs shaping the field this year, highlighting their unique strengths and optimal use cases.\\n\\n### GPT-5: Unified Multimodal Architecture  \\nGPT-5 represents a significant leap with its integrated architecture that supports text, voice, image, and video modalities natively. This unified design allows for seamless cross-modal understanding and generation, enabling complex multimodal interactions such as video captioning with voice context or dynamic image-text conversations. GPT-5’s approach improves efficiency by sharing underlying representations across modalities, making it highly suitable for interactive AI assistants and content creation platforms requiring fluid modality switching ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n### Meta’s Llama 4 Scout: Scalable Long-Context Processing  \\nMeta’s Llama 4 Scout pushes the boundaries in scalability with 17 billion parameters coupled with a massive 10 million token context window. This extended context enables processing of extremely large documents, videos, or multi-turn dialogues, enhancing performance in tasks like long-form content analysis, document summarization, and complex multimodal reasoning. Its parameter size balances capability and computational efficiency, making it ideal for enterprise-level applications requiring deep contextual awareness over extensive datasets ([Top 15 Multimodal Models in 2026](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\n### DeepSeek’s 671B-Parameter Model with Mixture of Experts (MoE)  \\nDeepSeek has introduced a colossal 671 billion parameter model leveraging Mixture of Experts (MoE), which dynamically activates only relevant subsets of the network per input. This optimization dramatically reduces inference costs and enables multimodal attention mechanisms across text, image, and audio data. The model excels in large-scale search and recommendation systems where multimodal input fusion (such as combining image recognition with textual metadata) is crucial. Its ability to handle diverse and heterogeneous data sources makes it a leader in complex retrieval and personalization tasks ([Large Multimodal Models in 2026](https://research.aimultiple.com/large-multimodal-models/)).\\n\\n### Qwen3-VL and GLM-4.5V: Specialized Visual Reasoning and OCR  \\nQwen3-VL and GLM-4.5V models have carved out niches in visual question answering (VQA), optical character recognition (OCR), and 3D spatial reasoning. Their architectures support fine-grained visual-textual alignment, enabling accurate interpretation of complex scenes and documents. Qwen3-VL shows strengths in scenario understanding requiring visual context reasoning, while GLM-4.5V is optimized for extracting and reasoning over text embedded in images and 3D environments. These models suit applications such as assistive technologies for the visually impaired, automated document processing, and augmented reality interfaces ([Top 15 Multimodal Models in 2026](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\n### Google Gemini 3 Pro: Real-Time Audio-Visual API Integration  \\nGoogle’s Gemini 3 Pro leads the benchmarks in multimodal performance, notably through its real-time audio-visual API which integrates synchronized video, speech, and textual inputs. This model excels in live interaction scenarios, such as virtual conferencing, real-time translation, and dynamic content moderation. The low-latency processing and high accuracy of Gemini 3 Pro make it pivotal in applications demanding immediate multimodal comprehension and response ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n### Open-Source Models: Democratizing Access  \\nAlongside proprietary giants, open-source multimodal models have expanded significantly, offering developers and researchers accessible tools for innovation. Frameworks like OpenLLM and various models featured in community-driven releases provide competitive performance on visual question answering, text-image generation, and cross-modal embeddings. These open models foster experimentation and customization, helping lower the barrier to entry and catalyzing multidisciplinary research and niche application development ([BentoML Open-Source LLMs](https://www.bentoml.com/blog/navigating-the-world-of-open-source-large-language-models), [SiliconFlow Open Source Models](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\n---\\n\\nIn summary, the 2026 multimodal LLM ecosystem is characterized by an increasing scale of parameters, extended context windows, and sophisticated multimodal attention techniques. Proprietary models lead in benchmarks and integration for real-time applications, while specialized architectures target domain-specific challenges such as 3D reasoning and OCR. Open-source alternatives are simultaneously democratizing access, driving innovation across industries and research communities.\\n\\n## Applications Driving Adoption of Multimodal LLMs\\n\\nMultimodal large language models (LLMs) in 2026 are transforming industries by integrating and processing diverse data types—text, images, audio, and spatial information—to create richer, context-aware applications. Their expanding adoption is fueled by practical use cases across sectors such as healthcare, autonomous driving, retail, and digital networking, where traditional single-modality models fall short.\\n\\nIn healthcare, multimodal LLMs enable enhanced diagnostic assistance by combining medical imaging, patient notes, and genomics data to provide more precise interpretations. For example, models are deployed for automated radiology image captioning alongside electronic health record analysis, improving diagnostic workflows and patient outcomes. Autonomous driving systems benefit from real-time sensor fusion, where visual, lidar, and situational audio data are jointly analyzed, enhancing object detection and navigation decisions under complex environments. Retail applications leverage multimodal capabilities to generate fine-grained product descriptions from images while incorporating user reviews and inventory data, enabling personalized recommendations and improved search experiences. In digital networking, these models facilitate advanced content moderation and user interaction by analyzing video, audio conversations, and accompanying text for context-rich moderation and engagement.\\n\\nCore multimodal use cases driving adoption include image captioning and visual question answering (VQA), where models interpret complex visual scenes intertwined with text queries to furnish accurate and contextually relevant answers. Optical character recognition (OCR) combined with natural language processing enables extraction and semantic understanding of text within images or scanned documents, streamlining data digitization and retrieval. Long-context analysis allows these LLMs to reason over extended documents and multimedia streams, supporting comprehensive summarization, trend detection, and decision-making.\\n\\nThe strength of multimodal LLMs lies in their data fusion capabilities—seamlessly blending signals across modalities to yield insights unattainable from isolated data sources. By unifying text, images, audio, and spatial inputs within a single architecture, organizations gain holistic perspectives that improve predictive accuracy and user relevance. This fusion supports industry-specific analytics workflows, where multimodal understanding automates interpretive tasks, accelerates data-driven decisions, and enriches enterprise automation pipelines. For instance, marketing analytics may combine customer sentiment from textual reviews with visual brand exposure metrics and audio ad campaign feedback to optimize strategies.\\n\\nReal-time agentic AI frameworks represent a frontier in multimodal applications, enabling responsive and interactive systems that process audio-visual inputs dynamically. These frameworks support conversational agents, virtual assistants, and autonomous agents capable of perceiving and acting upon complex sensory information with low latency, fostering new forms of human-computer interaction and immersion.\\n\\nDespite these advances, several challenges temper rapid adoption at scale. Integrating heterogeneous data sources demands sophisticated preprocessing and alignment strategies to maintain semantic coherence. Storage and compute infrastructures face pressures due to the increased model size and multimodal data complexity, impacting deployment feasibility. Moreover, latency in multimodal fusion and inference pipelines constrains responsiveness, especially for time-sensitive applications like autonomous driving or real-time virtual assistants. Addressing these bottlenecks remains critical to fully unlocking the transformative potential of multimodal LLMs across industries.\\n\\nReferences:\\n\\n- Applications and industry trends overview: [Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)  \\n- Use cases including VQA and OCR: [The NineHertz Multimodal Use Cases](https://theninehertz.com/blog/multimodal-ai-use-cases)  \\n- Data fusion methodologies: [ICANN 2026 on Multimodal Data Fusion](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)  \\n- Agentic AI frameworks enabling real-time interaction: [Adaline’s Top Agentic Models](https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026)  \\n- Enterprise adoption challenges and market outlook: [Straits Research Enterprise LLM Market](https://straitsresearch.com/report/enterprise-llm-market)\\n\\n## Performance, Scalability, and Deployment Considerations\\n\\nIn 2026, the performance and deployment of multimodal large language models (LLMs) are shaped by advances in architecture, optimization techniques, and system design, addressing the inherent complexity of processing multi-source inputs like text, images, audio, and video.\\n\\nA key trend is the use of mixture-of-experts (MoE) architectures to improve scalability while reducing compute costs. MoE enables models to dynamically activate only relevant expert subnetworks, which lowers resource consumption compared to fully dense models of comparable size. This selective activation makes extremely large model sizes more practical to deploy and scale, especially in cloud environments, by balancing accuracy gains against computational overhead ([SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\nQuantization at INT4 precision has become a mainstream technique to efficiently run these massive multimodal models on single GPUs with limited VRAM. INT4 quantization reduces memory footprint roughly fourfold versus full-precision, enabling real-time inference in edge and on-prem scenarios without drastically sacrificing accuracy. This approach lowers barriers for deploying sophisticated multimodal inference in latency-sensitive applications such as interactive assistants and AR/VR agents ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nExtended context windows have transformed multimodal understanding and generation, allowing models to process longer sequences of combined modalities jointly. Models supporting context sizes exceeding 8K tokens better capture temporal and spatial dependencies across modalities. However, larger context lengths incur significantly higher memory usage and computational costs, demanding innovations in memory-efficient attention mechanisms to maintain practical performance ([UnitLab AI](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\nStreaming and real-time inference capabilities have improved substantially, driven by the rising demand in agentic AI systems that interact continuously with users and environments. Incremental modality fusion and partial output generation reduce inference latency, supporting frameworks where multimodal models act as persistent, context-aware agents ([Adaline AI](https://www.adaline.ai/blog/top-agentic-llm-models-frameworks-for-2026)).\\n\\nDespite these advances, critical challenges remain in managing latency and throughput when handling heterogeneous multimodal streams at scale. The need to synchronize asynchronous modalities — for example, aligning image and audio data while maintaining responsiveness — complicates pipeline design. Scaling to high inference rates requires optimized batching strategies and hardware-aware scheduling to address memory bandwidth bottlenecks.\\n\\nDebugging and observability in multimodal inference pipelines require specialized tooling. Modality fusion errors, such as misaligned embeddings or modality dropout, can degrade output quality silently. Practitioners rely on granular monitoring of per-modality attention maps, intermediate embedding distributions, and cross-modal consistency scores to diagnose issues. Integrating modality-specific performance counters with end-to-end latency tracing is essential for maintaining robust deployment environments ([ICANN 2026](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nIn summary, the combination of MoE architectures, INT4 quantization, expanded context handling, and streaming inference advances has propelled multimodal LLMs toward scalable, deployable systems in 2026. Yet, overcoming latency, throughput, and observability challenges remains a critical frontier for research and engineering teams aiming to operationalize these models at production scale.\\n\\n## Challenges and Future Directions for Multimodal LLMs\\n\\nMultimodal large language models (LLMs) in 2026 face significant technical and practical hurdles, even as their applications expand rapidly across diverse domains. One core challenge lies in effectively integrating heterogeneous data modalities—such as text, images, audio, and video—into a seamless, coherent representation. This fusion is complicated by differing data formats, temporal and spatial misalignments, and modality-specific noise, which can degrade model performance if not carefully addressed. Research at ICANN 2026 highlights ongoing efforts in advanced fusion architectures that dynamically weight modal inputs based on context, but perfecting cross-modal synergy remains an open problem ([Source](https://e-nns.org/icann2026/multimodal-data-fusion-with-large-language-models/)).\\n\\nData storage and privacy concerns also intensify in the multimodal realm. Multimodal datasets, especially real-time streams combining video, audio, and sensor data, require enormous storage and bandwidth resources. More critically, they often contain sensitive personal information spread across modalities, raising unique privacy risks. Techniques like federated learning and differential privacy tailored for multimodal setups are emerging but are not yet widely standardized, complicating compliance and trust in deployed systems ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nInterpretability and debugging become exponentially more complex as model architectures grow in multimodal scope. Traditional LLMs already challenge developers to trace reasoning steps; adding visual or auditory components increases the dimensionality of latent spaces and potential failure points. Understanding how different modalities influence decisions and identifying errors requires new multimodal explainability tools that can disentangle cross-modal interactions—an active but nascent research area ([Source](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)).\\n\\nEnergy consumption and environmental impacts are growing concerns as multimodal LLMs scale to trillions of parameters. Processing multiple data streams demands more compute cycles and memory bandwidth, often in continuous, real-time deployments. The carbon footprint of training and inference must be balanced against application benefits. Industry trends show a gradual pivot toward modular architectures and lightweight models that operate efficiently within specialized domains, such as medical imaging or robotics, reducing unnecessary computational overhead ([Source](https://blog.unitlab.ai/top-multimodal-models/)).\\n\\nLooking forward, modular and lightweight multimodal models appear poised to dominate development. These smaller, adaptable units can be combined or fine-tuned to specific tasks, enabling faster iteration and deployment without the cost of monolithic models. Additionally, breakthroughs in unsupervised and continual learning hold promise for improving cross-modal generalization. By enabling models to learn from unlabelled, evolving data streams, these approaches can reduce dependency on large annotated datasets and adapt to domain shifts dynamically—critical for real-world applications in ever-changing environments ([Source](https://www.tiledb.com/blog/multimodal-ai-models)).\\n\\nIn summary, while multimodal LLMs have made remarkable progress, their future hinges on overcoming integration complexity, safeguarding privacy, enhancing interpretability, and mitigating environmental costs. Advances in modular design and learning paradigms will be essential to unlock their full potential across specialized and generalist AI systems.\\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#run(\"Write a blog on Open Source LLMs in 2026\")\n",
    "run(\"State of Multimodal LLMs in 2026\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b188c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
